
\documentclass[12pt]{article}
\usepackage[a4paper,margin=0.7in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{url}
\usepackage{pgfgantt}
\usepackage{titlesec}
\bibliographystyle{IEEEtran}
\usepackage{titlesec}

\titlespacing*{\section}
{0pt}{*0.8}{*0.65}

\setstretch{1.15}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}

\begin{document}

\begin{center}
{\Large \textbf{Spam Email Classification using Transfer Learning and Transformer Models}}\\[8pt]
{\small \textbf{COMP 6321 - Project Proposal - Fall 2025}}\\[5pt]
{\small Ayda Nodel Hokmabadi, Saumya Rana, Zar Chi Phyo, Mounika Satya Vani Gundavarapu}\\[3pt]
\end{center}


\section{Introduction and Literature Review}

Detection of spam has remained a major challenge due to the large number of unwanted and malicious emails sent every day, which cause threats such as phishing, fraud, and information overload. In addition, traditional rule-based filters are not usually successful against the constantly evolving spam methods~\cite{banday2009effectiveness,wang2025review}. Therefore, spam filtering has evolved from static rule-based systems to supervised machine learning approaches such as \textit{Naive Bayes}, \textit{Logistic Regression}, and \textit{Support Vector Machines}, trained on \textit{bag-of-words} and \textit{TF--IDF}\footnote{Term Frequency–Inverse Document Frequency} features over time~\cite{hassan2022analytics,chavez2020nb}. These models improved accuracy and interpretability but largely ignored word order and semantic context.

With larger datasets and greater computational power, deep learning models such as \textit{Recurrent Neural Networks (RNNs)} and \textit{Long Short-Term Memory (LSTM)} networks preserved sequential dependencies and often outperformed classical methods, albeit with higher data and computational requirements~\cite{johnafrica2022lstm,nasreen2024novel}. More recently, \textit{Transformer}-based architectures, such as \textit{BERT}\footnote{Bidirectional Encoder Representations from Transformers} and \textit{DistilBERT}, leverage the \textit{self-attention mechanism} to capture long-range semantic relationships and enable effective \textit{transfer learning} on relatively small labeled corpora for email security tasks, including spam and phishing detection~\cite{tida2022universal,songailaite2023bertphish,asliyuksek2025multimodal}.

\section{Motivations and Objectives}
This project aims to implement a spam classification system that compares classical and modern learning models. Naive Bayes and Logistic Regression will serve as baselines, while transfer learning with pre-trained BERT and DistilBERT models will represent the modern approach. By comparing these methods, the project seeks to evaluate how transfer learning improves spam detection in terms of accuracy, precision, recall, and F1-score, contributing to more effective and context-aware email filtering.

\section{Dataset Description}
In this project, we use the public \texttt{Bourique/data\_email\_spam} corpus available on Hugging Face \cite{huggingface_dataset}, which provides a single split with approximately $ 85.8\text{k}$ emails in CSV format. Each row has the text field \texttt{text\_combined} (subject + body) and a binary \texttt{label} (0 = ham, 1 = spam). We will split the data into 75\% train and 25\% test. In our project, \texttt{text\_combined} is the model input and \texttt{label} is the ground-truth target.

\section{ Methodology}
This project designs a spam detector and compares traditional machine learning methods with advanced transformer-based models. For the proposed model, we implement transfer learning using pre-trained BERT and DistilBERT models from the Hugging Face library. These models are fine-tuned on the spam email dataset by adding a fully connected classification layer on top of the Transformer encoder. In the preprocessing step, each input email is tokenized using the WordPiece tokenizer, truncated or padded to a fixed sequence length, and passed through the encoder to generate contextual embeddings. The hidden representation corresponding to the [CLS] token is then fed into a dense layer with a sigmoid activation for binary spam–ham classification.
The fine-tuning process uses the AdamW optimizer with early stopping based on validation F1-score. For comparison, the baseline models will be trained on TF–IDF feature representations extracted from the same dataset. Evaluation metrics will include accuracy, precision, recall, and F1-score.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{modell.jpg}
    \caption{ Project Methodology}
    \label{model}
\end{figure}

\section{Timeline}
We will carry out the project in the following steps:

\begin{center}
\resizebox{\linewidth}{!}{%
\begin{ganttchart}[
  time slot format=isodate,
  x unit=0.38cm,
  y unit chart=0.34cm,
  bar height=0.32,
  vgrid, hgrid,
  title height=0.5,
  title label font=\scriptsize,
  bar label font=\scriptsize,
  bar/.append style={draw=black!60, fill=blue!25},
  progress label text={}
]{2025-10-08}{2025-11-30}
  \gantttitlecalendar{month=shortname, week} \\

  % Data set processing: Oct 8–Oct 20
  \ganttbar[bar/.append style={fill=blue!40}]{Data preprocessing}{2025-10-08}{2025-10-20} \\
  
  \ganttbar[bar/.append style={fill=teal!40}]{Model and baselines implementation}{2025-10-21}{2025-11-10} \\
  
  \ganttbar[bar/.append style={fill=purple!40}]{Progress report submission}{2025-11-02}{2025-11-02} \\

  \ganttbar[bar/.append style={fill=orange!50}]{Evaluation}{2025-11-10}{2025-11-23} \\

  \ganttbar[bar/.append style={fill=red!55}]{Final report writing}{2025-11-24}{2025-11-29} \\

  \ganttbar[bar/.append style={fill=green!60}]{Final report submission}{2025-11-30}{2025-11-30}

\end{ganttchart}%
}
\end{center}


\newpage

\bibliography{references}
\end{document}
