\documentclass[12pt]{article}
\usepackage[a4paper,margin=0.8in]{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{url}
\usepackage{pgfgantt}
\usepackage{titlesec}
\bibliographystyle{IEEEtran}


\setstretch{1.15}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}

\begin{document}

\begin{center}
{\Large \textbf{Spam Email Classification using Transfer Learning and Transformer Models}}\\[8pt]
{\small \textbf{COMP 6321 - Project Proposal - Fall 2025}}\\[5pt]
{\small Ayda Nodel Hokmabadi, Saumya Rana, Zar Chi Phyo, Mounika Satya Vani Gundavarapu}\\[3pt]
\end{center}


\section{Introduction and Literature Review}
The problem of spam detection remains highly relevant due to the large number of malicious or unwanted emails sent daily, which pose risks such as phishing, fraud, and information overload. Traditional rule-based filters often fail against evolving spam techniques. Traditional rule-based filters degrade as spammers change wording and structure to evade fixed patterns \cite{banday2009effectiveness,wang2025review}.

Over time, spam filtering has shifted from rules to supervised machine learning with Naive Bayes, Logistic Regression, and Support Vector Machines trained on bag-of-words and TF--IDF features \cite{hassan2022analytics,chavez2020nb}. These models improved accuracy and interpretability but largely ignore word order and semantic context.

With larger datasets and greater compute, deep learning models such as RNNs and LSTMs preserved sequential dependencies and often outperformed classical methods, though at higher data and computational cost \cite{johnafrica2022lstm,nasreen2024novel}. More recently, Transformer-based architectures (e.g., BERT and DistilBERT) use self-attention to capture long-range semantic relationships and enable effective transfer learning on relatively small labeled corpora for email security tasks, including spam and phishing detection \cite{tida2022universal,songailaite2023bertphish,asliyuksek2025multimodal}.

This project aims to develop a spam classification system that compares classical and modern learning models. Naive Bayes and Logistic Regression will serve as baselines, while transfer learning with pre-trained BERT and DistilBERT models will represent the modern approach. By comparing these methods, the project seeks to evaluate how transfer learning improves spam detection in terms of accuracy, precision, recall, and F1-score, contributing to more effective and context-aware email filtering.

\section{Dataset Description}
In this project, we use the public \texttt{bourigue/data\_email\_spam} corpus available on Hugging Face \cite{huggingface_dataset}, which provides a single split with approximately $ 85.8\text{k}$ emails in CSV format. Each row has the text field \texttt{text\_combined} (subject + body) and a binary \texttt{label} (0 = ham, 1 = spam). We will split the data into 75\% train and 25\% test. In our project, \texttt{text\_combined} is the model input and \texttt{label} is the ground-truth target.

\section{ Methodology}
The project will compare both traditional and modern models for spam email detection. The proposed approach is based on transfer learning using pre-trained BERT and DistilBERT models, which capture contextual and semantic relationships in text. To provide meaningful benchmarks, we will also implement Naive Bayes and Logistic Regression as baseline classifiers. More specifically, Naive Bayes serves as a simple yet effective probabilistic model for text classification, while Logistic Regression operates as a linear model leveraging TF–IDF features to represent weighted word importance. By contrasting these baselines with the Transformer-based models, the project aims to evaluate the performance gains brought by transfer learning in terms of accuracy, precision, recall, and F1-score.


\section{Timeline}
We will carry out the project in the following steps:

\begin{center}
\resizebox{\linewidth}{!}{%
\begin{ganttchart}[
  time slot format=isodate,
  x unit=0.38cm,
  y unit chart=0.34cm,
  bar height=0.32,
  vgrid, hgrid,
  title height=0.5,
  title label font=\scriptsize,
  bar label font=\scriptsize,
  bar/.append style={draw=black!60, fill=blue!25},
  progress label text={}
]{2025-10-08}{2025-11-30}
  \gantttitlecalendar{month=shortname, week} \\

  % Data set processing: Oct 8–Oct 20
  \ganttbar[bar/.append style={fill=blue!40}]{Data preprocessing}{2025-10-08}{2025-10-20} \\
  
  \ganttbar[bar/.append style={fill=teal!40}]{Model and baselines implementation}{2025-10-21}{2025-11-10} \\
  
  \ganttbar[bar/.append style={fill=purple!40}]{Progress report submission}{2025-11-02}{2025-11-02} \\

  \ganttbar[bar/.append style={fill=orange!50}]{Evaluation}{2025-11-10}{2025-11-23} \\

  \ganttbar[bar/.append style={fill=red!55}]{Final report writing}{2025-11-24}{2025-11-29} \\

  \ganttbar[bar/.append style={fill=green!60}]{Final report submission}{2025-11-30}{2025-11-30}

\end{ganttchart}%
}
\end{center}


\newpage

\bibliography{references}
\end{document}
